---
title: "hclustext"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  md_document:
    toc: true      
---

```{r, echo=FALSE}
library(knitr)
desc <- suppressWarnings(readLines("DESCRIPTION"))
regex <- "(^Version:\\s+)(\\d+\\.\\d+\\.\\d+)"
loc <- grep(regex, desc)
ver <- gsub(regex, "\\2", desc[loc])
verbadge <- sprintf('<a href="https://img.shields.io/badge/Version-%s-orange.svg"><img src="https://img.shields.io/badge/Version-%s-orange.svg" alt="Version"/></a></p>', ver, ver)
````


```{r, echo=FALSE}
knit_hooks$set(htmlcap = function(before, options, envir) {
  if(!before) {
    paste('<p class="caption"><b><em>',options$htmlcap,"</em></b></p>",sep="")
    }
    })
knitr::opts_knit$set(self.contained = TRUE, cache = FALSE)
knitr::opts_chunk$set(fig.path = "inst/figure/")
```

[![Project Status: Active - The project has reached a stable, usable state and is being actively developed.](http://www.repostatus.org/badges/latest/active.svg)](http://www.repostatus.org/#active)
[![Build Status](https://travis-ci.org/trinker/hclustext.svg?branch=master)](https://travis-ci.org/trinker/hclustext)
[![Coverage Status](https://coveralls.io/repos/trinker/hclustext/badge.svg?branch=master)](https://coveralls.io/r/trinker/hclustext?branch=master)
`r verbadge`

<img src="inst/hclustext_logo/r_hclustext.png" width="150" alt="readability Logo">

**hclustext** is a collection of optimized tools for clustering text data via hierarchical clustering.  There are many great R [clustering tools](https://cran.r-project.org/web/views/Cluster.html) to locate topics within documents.  I have had success with hierarchical clustering for topic extraction.  This package wraps many of the great R tools for clustering and working with sparse matrices to aide in the workflow associated with topic extraction.

The general idea is that we turn the documents into a matrix of words.  After this we weight the terms by importance using [tf-idf](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html).  This helps the more salient words to rise to the top.  We then apply cosine distance measures to compare the terms (or features) of each document.  Cosine distance works well with sparse matrices to produce distances metrics between the documents.  The hierarchical clustering is fit to separate the documents into clusters.  The user then may apply k clusters to the fit, clustering documents with similar important text features.  The documents can then be grouped by clusters and their accompanying salient words extracted as well.

# Functions

The main functions, task category, & descriptions are summarized in the table below:

| Function               |  Category      | Description                                                            |
|------------------------|----------------|------------------------------------------------------------------------|
| `data_store`           | data structure | **hclustext**'s data structure (list of dtm + text)                    |
| `hierarchical_cluster` | cluster fit    | Fits a hierarchical cluster model                                      |
| `assign_cluster`       | assignment     | Assigns cluster to document/text element                               |
| `get_text`             | extraction     | Get text from various **hclustext** objects                            |
| `get_dtm`              | extraction     | Get `tm::DocumentTermMatrix` from various **hclustext** objects        |
| `get_removed`          | extraction     | Get removed text elements from various **hclustext** objects           |
| `get_terms`            | extraction     | Get weighted important terms from an **assign_cluster** object         |



# Installation

To download the development version of **hclustext**:

Download the [zip ball](https://github.com/trinker/hclustext/zipball/master) or [tar ball](https://github.com/trinker/hclustext/tarball/master), decompress and run `R CMD INSTALL` on it, or use the **pacman** package to install the development version:

```r
if (!require("pacman")) install.packages("pacman")
pacman::p_load_gh(
    "trinker/textshape", 
    "trinker/gofastr", 
    "trinker/termco",    
    "trinker/hclustext"
)
```

# Contact

You are welcome to:    
* submit suggestions and bug-reports at: <https://github.com/trinker/hclustext/issues>    
* send a pull request on: <https://github.com/trinker/hclustext/>      
* compose a friendly e-mail to: <tyler.rinker@gmail.com>     

# Demonstration

## Load Packages and Data

```{r}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(hclustext, dplyr, textshape)

data(presidential_debates_2012)
```


## Data Structure

The data structure for **hclustext** is very specific.  The `data_storage` produces a `DocumentTermMatrix` which maps to the original text.  The empty/removed documents are tracked within this data structure, making subsequent calls to cluster the original documents and produce weighted important terms more robust.  Making the `data_storage` object is the first step to analysis.

We can give the `DocumentTermMatrix` rownames via the `doc.names` argument.  If these names are not unique they will be combined into a single document as seen below.  Also, if you want to do stemming, minimum character length, stopword removal or such this is when/where it's done.


```{r}
ds <- with(
    presidential_debates_2012,
    data_store(dialogue, doc.names = paste(person, time, sep = "_"))
)

ds
```


## Fit the Model: Hierarchical Cluster

Next we can fit a hierarchical cluster model to the `data_store` object via `hierarchical_cluster`.

```{r}
myfit <- hierarchical_cluster(ds)

myfit
```


This object can be plotted with various `k` or `h` paremeters specified to experiment with cutting the dendrogram.  This cut will determine the number of clusters or topics that will be generated in the next step.  The visual inspection allows for determining how to cluster the data as well as determining if a tf-idf, cosine, hierarchical cluster model is a right fit for the data and task.  By default `plot` uses an approximation of `k` based on Can & Ozkarahan's (1990) formula $(m * n)/t$ where $m$ and $n$ are the dimensions of the matrix and $t$ is the length of the non-zero elements in matrix $A$.

- Can, F., Ozkarahan, E. A. (1990). Concepts and effectiveness of the cover-coefficient-based clustering methodology for text databases. *ACM Transactions on Database Systems 15* (4): 483. doi:10.1145/99935.99938

Interestingly, in the plots below where `k = 6` clusters, the model groups each of the candidates together at each of the debate times.


```{r}
plot(myfit)
plot(myfit, k=6)
plot(myfit, h = .75)
```

## Assigning Clusters

```{r}
ca <- assign_cluster(myfit, k = 6)

ca
```


### Cluster Loading


```{r}
summary(ca)
```

### Cluster Text 

```{r}
get_text(ca)
## Need to condense data_store text if dtm is condensed
```

### Cluster Frequent Terms

```{r}
get_terms(ca)
```

## Putting it Together

I like working in a chain.  In the setup below we work within a **magrittr** pipeline to fit a model, select clusters, and examine the results.  In this example I do not condense the 2012 Presidential Debates data by speaker and timer, rather leaving every sentence as a separate document.  On my machine the initial `data_store` and model fit takes ~5-8 seconds to crun.


```{r}
.tic <- Sys.time()

myfit2 <- presidential_debates_2012 %>%
    with(data_store(dialogue)) %>%
    hierarchical_cluster()

difftime(Sys.time(), .tic)

## View Document Loadings
ca2 <- assign_cluster(myfit2, k = 100)
summary(ca2)

## Split Text into Clusters
set.seed(3); inds <- sort(sample.int(100, 5))
get_text(ca2)[inds] %>%
    lapply(head, 10)

## Get associated terms
get_terms(ca2)[inds]
```



